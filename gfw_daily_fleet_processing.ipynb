{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "from arcgis import GIS\n",
    "import os\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "from arcgis.features import GeoAccessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define variables\n",
    "# connect to GIS\n",
    "gis = GIS()\n",
    "\n",
    "# directory where the daily fleet csv files from GFW are stored\n",
    "directory = r'F:\\Global Fishing Watch\\Fishing Intensity\\v2\\fleet-daily\\fleet-daily-csvs-100-v2-2020'\n",
    "processing_gdb = r'E:\\analysis\\GFW\\gfw_daily_fleet_2020.gdb'\n",
    "analysis_directory = r'E:\\analysis\\GFW\\TIF\\2020'\n",
    "\n",
    "# To allow overwriting outputs change overwriteOutput option to True.\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.parallelProcessingFactor = \"90%\"\n",
    "\n",
    "# Check out any necessary licenses.\n",
    "arcpy.CheckOutExtension(\"spatial\")\n",
    "arcpy.CheckOutExtension(\"ImageAnalyst\")\n",
    "\n",
    "projectCoordinateSystem=\"GEOGCS[\\\"GCS_WGS_1984\\\",DATUM[\\\"D_WGS_1984\\\",SPHEROID[\\\"WGS_1984\\\",6378137.0,298.257223563]],PRIMEM[\\\"Greenwich\\\",0.0],UNIT[\\\"Degree\\\",0.0174532925199433]]\"\n",
    "gear_list = ['fishing', 'drifting_longlines', 'seiners', 'purse_seines', 'tuna_purse_seines', 'other_purse_seines', 'other_seines', 'trollers', 'fixed_gear', 'pots_and_traps', 'set_longlines', 'set_gillnets', 'dredge_fishing', 'squid_jiggers', 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories():\n",
    "    # if not arcpy.Exists(processing_gdb):\n",
    "    #     arcpy.management.CreateFileGDB(os.path.dirname(processing_gdb), os.path.basename(processing_gdb))\n",
    "    if not os.path.exists(analysis_directory):\n",
    "        os.makedirs(analysis_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "            print(\"Starting processing of \" + filename + \"...\")\n",
    "            # Get the full path for the filename\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            # Read the CSV file\n",
    "            print(file_path)\n",
    "            global data\n",
    "            data = pd.read_csv(file_path)\n",
    "            # Get the first row as a table\n",
    "            first_row = data.head(1)\n",
    "            # Get the value in the first column\n",
    "            global value\n",
    "            value = first_row.iloc[0, 0]\n",
    "            # replace the '-' with '_'\n",
    "            value = value.replace('-', '_')\n",
    "            global tif_name1\n",
    "            tif_name1 = os.path.join(analysis_directory, \"daily_fleet_\" + value + \"_hours.tif\")\n",
    "            global fc_hours\n",
    "            fc_hours = os.path.join(processing_gdb, \"daily_fleet_\" + value + \"_hours\")\n",
    "            global tif_name2\n",
    "            tif_name2 = os.path.join(analysis_directory, \"daily_fleet_\" + value + \"_fishhours.tif\")\n",
    "            data['cell_ll_lat'] = data['cell_ll_lat'] + 0.05\n",
    "            data['cell_ll_lon'] = data['cell_ll_lon'] + 0.05\n",
    "            data.rename(columns={'cell_ll_lat': 'lat'}, inplace=True)\n",
    "            data.rename(columns={'cell_ll_lon': 'lon'}, inplace=True)\n",
    "            print(\"Done with \" + value + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hours():\n",
    "    hours_select = data.loc[data['hours'] != 0]\n",
    "    hours_csv = os.path.join(analysis_directory, \"daily_fleet_\" + value + \"_hours.csv\")\n",
    "    hours_select.drop(columns=['date', 'flag', 'geartype', 'fishing_hours', 'mmsi_present'], inplace=True)\n",
    "    hours_select.to_csv(hours_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fishhours():\n",
    "    fishhours_select = data.loc[data['fishing_hours'] != 0]\n",
    "    fishhours_csv = os.path.join(analysis_directory, \"daily_fleet_\" + value + \"_fishhours.csv\")\n",
    "    fishhours_select.drop(columns=['date', 'flag', 'geartype', 'hours', 'mmsi_present'], inplace=True)\n",
    "    fishhours_select.to_csv(fishhours_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directories()\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        preprocess()\n",
    "        process_hours()\n",
    "        process_fishhours()\n",
    "print(\"All done...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(analysis_directory):\n",
    "    if filename.endswith('fishhours.csv'):\n",
    "        csv_name = (os.path.basename(filename))\n",
    "        print(\"Processing \" + csv_name + \"...\")\n",
    "        fish_csv = (os.path.join(analysis_directory, os.path.splitext(filename)[0] + \".csv\"))\n",
    "        # print(fish_csv)\n",
    "        fish_tif = (os.path.join(analysis_directory, os.path.splitext(filename)[0] + \".tif\"))\n",
    "        # print(fish_tif)\n",
    "        fish_fc = (os.path.join(processing_gdb, os.path.splitext(os.path.basename(filename))[0]))\n",
    "        # print(fish_fc)\n",
    "        arcpy.management.XYTableToPoint(fish_csv, fish_fc, 'lon', 'lat', '', projectCoordinateSystem)\n",
    "        arcpy.conversion.PointToRaster(fish_fc, 'fishing_hours', fish_tif, \"SUM\", 'fishing_hours', 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(analysis_directory):\n",
    "    if filename.endswith('_hours.csv'):\n",
    "        csv_name = (os.path.basename(filename))\n",
    "        print(\"Processing \" + csv_name + \"...\")\n",
    "        hours_csv = (os.path.join(analysis_directory, os.path.splitext(filename)[0] + \".csv\"))\n",
    "        # print(hours_csv)\n",
    "        hours_tif = (os.path.join(analysis_directory, os.path.splitext(filename)[0] + \".tif\"))\n",
    "        # print(hours_tif)\n",
    "        hours_fc = (os.path.join(processing_gdb, os.path.splitext(os.path.basename(filename))[0]))\n",
    "        # print(hours_fc)\n",
    "        arcpy.management.XYTableToPoint(hours_csv, hours_fc, 'lon', 'lat', '', projectCoordinateSystem)\n",
    "        arcpy.conversion.PointToRaster(hours_fc, 'hours', hours_tif, \"SUM\", 'hours', 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a mosaic dataset\n",
    "mosacic_name = 'global_fish_watch_daily_fleet_sum_2000'\n",
    "mosaic_path = os.path.join(processing_gdb, mosacic_name)\n",
    "\n",
    "arcpy.management.CreateMosaicDataset(in_workspace = processing_gdb,\n",
    "                                     in_mosaicdataset_name = mosacic_name,\n",
    "                                     coordinate_system = projectCoordinateSystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arcpy.management.AddRastersToMosaicDataset(in_mosaic_dataset = mosaic_path,\n",
    "                                           raster_type = \"Raster Dataset\",\n",
    "                                           input_path = analysis_directory, \n",
    "                                           sub_folder=\"SUBFOLDERS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.management.CalculateField(\n",
    "    in_table=mosaic_path,\n",
    "    field=\"date\",\n",
    "    expression=\"\"\"(!Name!.split('_')[3] + \"-\" + !Name!.split('_')[4] + \"-\" + !Name!.split('_')[2])\"\"\",\n",
    "    expression_type=\"PYTHON3\",\n",
    "    code_block=\"\",\n",
    "    field_type=\"DATE\",\n",
    "    enforce_domains=\"NO_ENFORCE_DOMAINS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.management.CalculateField(\n",
    "    in_table=mosaic_path,\n",
    "    field=\"variable\",\n",
    "    expression=\"\"\"(!Name!.split('_')[5].split('.')[0])\"\"\",\n",
    "    expression_type=\"PYTHON3\",\n",
    "    code_block=\"\",\n",
    "    field_type=\"TEXT\",\n",
    "    enforce_domains=\"NO_ENFORCE_DOMAINS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosacic_name = 'global_fish_watch_daily_fleet_sum_2000'\n",
    "mosaic_path = os.path.join(processing_gdb, mosacic_name)\n",
    "\n",
    "arcpy.md.BuildMultidimensionalInfo(in_mosaic_dataset = mosaic_path,\n",
    "                                   variable_field = 'variable',\n",
    "                                   dimension_fields = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with arcpy.EnvManager(parallelProcessingFactor=\"99%\"):\n",
    "    arcpy.management.CalculateStatistics(in_raster_dataset = mosaic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with arcpy.EnvManager(compression=\"'LERC' 0.010000\", parallelProcessingFactor=\"99%\", pyramid=\"PYRAMIDS -1 NEAREST JPEG 85 NO_SKIP NO_SIPS\"):\n",
    "    arcpy.management.CopyRaster(\n",
    "        in_raster=mosaic_path,\n",
    "        out_rasterdataset=os.path.join(analysis_directory, mosacic_name + \".crf\"),\n",
    "        config_keyword=\"\",\n",
    "        background_value=None,\n",
    "        nodata_value=\"\",\n",
    "        onebit_to_eightbit=\"NONE\",\n",
    "        colormap_to_RGB=\"NONE\",\n",
    "        pixel_type=\"\",\n",
    "        scale_pixel_value=\"NONE\",\n",
    "        RGB_to_Colormap=\"NONE\",\n",
    "        format=\"CRF\",\n",
    "        transform=\"NONE\",\n",
    "        process_as_multidimensional=\"ALL_SLICES\",\n",
    "        build_multidimensional_transpose=\"TRANSPOSE\"\n",
    "        # build_multidimensional_transpose=\"NO_TRANSPOSE\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial commit made"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
